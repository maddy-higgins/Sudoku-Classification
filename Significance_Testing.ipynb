{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZjOLnwtUyg"
      },
      "source": [
        "# Statistical Techniques for Classifiers Comparison withÂ Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkS9xxVpEqx"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import os\n",
        "from sklearn import model_selection\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducability\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "rn.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "twWtwAXuKN2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30swfqoiZQrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28922fa-58f1-4d12-c50b-c22868884367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "train_data, test_data = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate((train_data[0], test_data[0]))\n",
        "Y = np.concatenate((train_data[1], test_data[1]))"
      ],
      "metadata": {
        "id": "BLUKAYFUENIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "X, Y, test_size=0.15, random_state=42, stratify = Y)"
      ],
      "metadata": {
        "id": "hfGcU7y0EWAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train val split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "train_images, train_labels, test_size=0.15, random_state=50, stratify = train_labels)"
      ],
      "metadata": {
        "id": "FiN8-Lv8E5H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq8aDetNuRK5"
      },
      "outputs": [],
      "source": [
        "# Reshape the labels and encode them categorically.\n",
        "y_train = tf.keras.utils.to_categorical(train_labels)\n",
        "y_test = tf.keras.utils.to_categorical(test_labels)\n",
        "y_val =  tf.keras.utils.to_categorical(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOxJv_26Zu2n"
      },
      "outputs": [],
      "source": [
        "# Reshape and normalize the images.\n",
        "X_train = train_images.reshape((train_images.shape[0], 784))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = test_images.reshape((test_images.shape[0], 784))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "X_val = val_images.reshape((val_images.shape[0], 784))\n",
        "X_val = X_val.astype('float32') / 255\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on Data"
      ],
      "metadata": {
        "id": "RY6xkE778H6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('sudoku_data_updated.csv')"
      ],
      "metadata": {
        "id": "5N2isMRPXKth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# be careful with this because if you run it multiple times, it will destroy the data\n",
        "data['difficulty'] = data['difficulty'] / 2\n",
        "data['difficulty'] = data['difficulty'].astype(int)"
      ],
      "metadata": {
        "id": "sP3F_EoWXc7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "lqODrnwQXQR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1 = pd.concat([data.iloc[:1800],data.iloc[5400:5600]], ignore_index=True) #difficulty low\n",
        "data_2 = pd.concat([data.iloc[1800:3600],data.iloc[5600:5800]]) #difficulty medium\n",
        "data_3 = pd.concat([data.iloc[3600:5400],data.iloc[5800:6000]]) # difficulty high\n"
      ],
      "metadata": {
        "id": "VGEypbpD_0bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using all\n",
        "X = data\n",
        "y = data['difficulty']\n",
        "X_1 = data_1\n",
        "y_1 = data_1['difficulty']\n",
        "X_2 = data_2\n",
        "y_2 = data_2['difficulty']\n",
        "X_3 = data_3\n",
        "y_3 = data_3['difficulty']\n",
        "\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=51)\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=52)\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.2, random_state=53)\n",
        "\n",
        "X_train, X_test, y_train, y_test = pd.concat([X_train_1,X_train_2,X_train_3], ignore_index=True),pd.concat([X_test_1,X_test_2,X_test_3],ignore_index=True), pd.concat([y_train_1,y_train_2,y_train_3],ignore_index=True),pd.concat([y_test_1,y_test_2,y_test_3],ignore_index=True)"
      ],
      "metadata": {
        "id": "HB0D6DeeYJrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRF5o3tj-j0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rf(features, X_train, y_train, X_test, y_test):\n",
        "  classifier = RandomForestClassifier(n_estimators=160, random_state=50)\n",
        "  classifier.fit(X_train[features], y_train)\n",
        "  y_pred = classifier.predict(X_test[features])\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "DBi_b3DlYu3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no solving strategies\n",
        "features = ['NoC','Poss','stddev','Var','r0','r1','r2','r3','Clusters']\n",
        "y_pred_1 = predict_rf(features, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "xKRRXYlrZQ5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all solving strategies\n",
        "features = ['NoC','Poss','stddev','Var','r0','r1','r2','r3','Clusters',\n",
        "            'NoC after pass', 'NoC after 2pass','NoC diff','NoC after passes',\n",
        "            'Pairs','Pair Red','Pair Red2','Pair Diff2','Pairs Poss']\n",
        "y_pred_3 = predict_rf(features, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "dFvnZ5Xy1U15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing one solving strat at a time\n",
        "variable = 'Pairs Poss'\n",
        "features = ['NoC','Poss','stddev','Var','r0','r1','r2','r3','Clusters',variable]\n",
        "y_pred_2 = predict_rf(features, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "lDt0bN6k__T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKqdIRBiwe6Q",
        "outputId": "cef8918c-3c0b-4107-c0d1-6bf62bb2eab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "col_0    0    1    2   All\n",
            "row_0                     \n",
            "0      256   77   77   410\n",
            "1       84  201  110   395\n",
            "2       66   93  236   395\n",
            "All    406  371  423  1200\n"
          ]
        }
      ],
      "source": [
        "ct = pd.crosstab(y_pred_1, y_pred_2, margins=True)\n",
        "print(ct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctab = ct.iloc[:-1,:-1]\n",
        "contingency_table = ctab.copy()\n",
        "print(contingency_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMprbEvOXsj",
        "outputId": "0710a0cb-8e4f-4de5-dfd9-004b8e0c8090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "col_0    0    1    2\n",
            "row_0               \n",
            "0      256   77   77\n",
            "1       84  201  110\n",
            "2       66   93  236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwyDMud15lWJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Code for running the tests\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def apply_sweep(x, MARGIN, FUN=np.subtract, STATS=None, check_margin=True, **kwargs):\n",
        "    '''\n",
        "    A Python version of the R function apply_sweep() from the basic R syntax.\n",
        "    The sweep R function applies an operation (e.g. + or -) to a data matrix by row or by column.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "    -----------------------\n",
        "    x: Typically a matrix.\n",
        "    MARGIN: Specifies typically whether the operation should be applied by row or by column. MARGIN = 1 operates by row; MARGIN = 2 operates by column.\n",
        "    STATS: Specifies usually the value that should be used for the operation (e.g. the value that should be added or subtracted).\n",
        "    FUN: The operation that should be carried out (e.g. + or -).\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    A matrix with the same dimensions as x with the operation applied by row or by column.\n",
        "    '''\n",
        "\n",
        "    if STATS is None:\n",
        "        STATS = np.array([], dtype=x.dtype)\n",
        "\n",
        "    dims = np.shape(x)\n",
        "\n",
        "    if isinstance(MARGIN, str):\n",
        "        dn = np.array(x).dtype.names\n",
        "\n",
        "        if dn is None:\n",
        "            raise ValueError(\"'x' must have named dimnames\")\n",
        "\n",
        "        MARGIN = np.where(dn == MARGIN)[0][0]\n",
        "\n",
        "        if np.any(np.isnan(MARGIN)):\n",
        "            raise ValueError(\"not all elements of 'MARGIN' are names of dimensions\")\n",
        "\n",
        "    if check_margin:\n",
        "        dimmargin = dims[MARGIN]\n",
        "        dimstats = pd.DataFrame(STATS).shape\n",
        "        lstats = np.size(STATS)\n",
        "\n",
        "        if lstats > np.prod(dimmargin):\n",
        "            print(\"Warning: STATS is longer than the extent of 'dim(x)[MARGIN]'\")\n",
        "        elif len(dimstats) == 0:\n",
        "            cumDim = np.append(1, np.cumprod(dimmargin))\n",
        "            upper = np.min(cumDim[cumDim >= lstats])\n",
        "            lower = np.max(cumDim[cumDim <= lstats])\n",
        "\n",
        "            if lstats and (upper % lstats != 0 or lstats % lower != 0):\n",
        "                print(\"Warning: STATS does not recycle exactly across MARGIN\")\n",
        "        else:\n",
        "            dimmargin = np.array(dimmargin).reshape(-1)\n",
        "            dimstats = np.array(dimstats)[:-1]\n",
        "\n",
        "            if len(dimstats) != len(dimmargin) or not np.any(dimstats == dimmargin):\n",
        "                print(\"Warning: length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\")\n",
        "\n",
        "    perm = np.concatenate((np.arange(MARGIN), np.arange(MARGIN+1, len(dims)), [MARGIN]))\n",
        "    return FUN(x, STATS).transpose(perm).T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# KOHEN'S KAPPA\n",
        "\n",
        "def kappa(x, weights=['Equal-Spacing', 'Fleiss-Cohen']):\n",
        "    '''\n",
        "    Computes two agreement rates: Cohen's kappa and weighted kappa, and confidence bands.\n",
        "    A Python version of the R function Kappa() from the vcd package.\n",
        "\n",
        "    Parameters:\n",
        "    -----------------------\n",
        "    x: array_like or pandas.DataFrame\n",
        "        If `x` is a matrix, it should be a KxK matrix of counts of\n",
        "        assignments of objects to K categories.\n",
        "    weights:veither one of the character strings given in the default value, or a user-specified matrix with same dimensions as x.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    A dictionary containing the following:\n",
        "    - 'Unweighted': numeric vector of length 2 with the kappa statistic (value component), along with Approximate Standard Error (ASE component).\n",
        "    - 'Weighted': idem for the weighted kappa.\n",
        "    - 'CI': matrix with lower and upper bounds of the 95% confidence interval for the unweighted and weighted kappa.\n",
        "\n",
        "    '''\n",
        "    if isinstance(weights, str):\n",
        "        weights = [weights]\n",
        "    d = np.diag(x)\n",
        "    n = np.sum(x).sum()\n",
        "    nc = x.shape[1]\n",
        "    colFreqs = np.sum(x, axis=0)/n\n",
        "    rowFreqs = np.sum(x, axis=1)/n\n",
        "\n",
        "    def kappa_val(po, pc):\n",
        "        return (po - pc)/(1 - pc)\n",
        "    def std(p, pc, kw, W=None):\n",
        "        dot1 = np.dot(W, np.sum(p, axis=0))\n",
        "        dot2 = np.dot(W, np.sum(p, axis=1) * (1 - kw))\n",
        "        inner1 = np.inner(1, 1-pc)\n",
        "        inner2 = np.inner(1-pc, 1)\n",
        "        sweep1 = apply_sweep( W, MARGIN = 1, STATS = dot1 * (1 - kw))\n",
        "        return  np.sqrt(((np.sum(p * apply_sweep(sweep1, MARGIN=0, STATS = dot2)**2)).sum()- (kw - pc * (1 - kw)) **2) /(inner1 * inner2)/n)\n",
        "\n",
        "    # unweighted kappa\n",
        "    po = np.sum(d)/n\n",
        "    pc = np.inner(colFreqs, rowFreqs)\n",
        "    k = kappa_val(po, pc)\n",
        "    W0 = np.diag(np.ones(nc))\n",
        "    s = std(x/n, pc, k, W0)\n",
        "\n",
        "    # weighted kappa\n",
        "    if isinstance(weights, list):\n",
        "        if weights[0] == 'Equal-Spacing':\n",
        "            W = 1 - np.abs(np.subtract.outer(np.arange(1, nc+1), np.arange(1, nc+1)))/(nc-1)\n",
        "        else:\n",
        "            W = 1 - np.power(np.abs(np.subtract.outer(np.arange(1, nc+1), np.arange(1, nc+1)))/(nc-1), 2)\n",
        "    else:\n",
        "        W = weights\n",
        "\n",
        "    pow = np.sum(W * x).sum()/n\n",
        "    pcw = np.sum(W * np.outer(colFreqs, rowFreqs))\n",
        "    kw = kappa_val(pow, pcw)\n",
        "    sw = std(x/n, pcw, kw, W)\n",
        "\n",
        "    #compute z-score and p-value\n",
        "    # tab <- rbind(x$Unweighted, x$Weighted)\n",
        "    tab = np.vstack((np.array([k, s]), np.array([kw, sw])))\n",
        "    z = tab[:,0] / tab[:,1]\n",
        "    # tab <- cbind(tab, z, `Pr(>|z|)` = 2 * pnorm(-abs(z)))\n",
        "    tab = np.c_[tab, z, 2 * norm.cdf(-abs(z))]\n",
        "\n",
        "    CI = True\n",
        "    if CI:\n",
        "        level = 0.95\n",
        "        q = norm.ppf((1 + level)/2)\n",
        "        lower = tab[:,1] - q * tab[:,2]\n",
        "        upper = tab[:,1] + q * tab[:,2]\n",
        "        tab_ci = np.c_[tab, lower, upper]\n",
        "\n",
        "\n",
        "    return {'Unweighted': {'kappa': k, 'std': s, 'z': z[0], 'p': tab[0,3]},\n",
        "            'Weighted': {'kappa': kw, 'std': sw, 'z': z[1], 'p': tab[1,3]},\n",
        "            'CI': {'lower': lower, 'upper': upper}}\n",
        "\n",
        "\n",
        "# STUART-MAXWELL TEST\n",
        "\n",
        "def StuartMaxwellTest(x, y=None, BhapkarTest=False):\n",
        "    \"\"\"\n",
        "    Stuart-Maxwell test for marginal homogeneity of a KxK matrix of assignments\n",
        "    of objects to K categories or an nx2 or 2xn matrix of category scores for n\n",
        "    data objects by two raters. The statistic is distributed as Chi-square with\n",
        "    K-1 degrees of freedom.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : array_like or pandas.DataFrame\n",
        "        If `x` is a matrix, it should be a KxK matrix of counts of\n",
        "        assignments of objects to K categories.\n",
        "    y : array_like, optional\n",
        "        If `x` is a data frame, `y` should not be provided. Otherwise,\n",
        "        `y` should be a KxK matrix with the same counts as `x`.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    result : dict\n",
        "        A dictionary containing the following:\n",
        "        - 'method': str, the name of the test\n",
        "        - 'data.name': str, the name of the data\n",
        "        - 'statistic': float, the test statistic\n",
        "        - 'parameter': int, the degrees of freedom\n",
        "        - 'p.value': float, the p-value of the test\n",
        "        - 'N': int, the total number of observations\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(x, np.ndarray or pd.DataFrame):\n",
        "        r = x.shape[0]\n",
        "        if r < 2 or x.shape[1] != r:\n",
        "            raise ValueError(\"'x' must be square with at least two rows and columns\")\n",
        "        if (x < 0).any().any() or np.isnan(x).any().any():\n",
        "            raise ValueError(\"all entries of 'x' must be nonnegative and finite\")\n",
        "        DNAME = \"x\"\n",
        "    else:\n",
        "        if y is None:\n",
        "            raise ValueError(\"if 'x' is not a matrix, 'y' must be given\")\n",
        "        if y is not None and not isinstance(y, np.ndarray or pd.DataFrame):\n",
        "            raise TypeError(\"'y' must be a matrix\")\n",
        "        if len(x) != len(y):\n",
        "            raise ValueError(\"'x' and 'y' must have the same length\")\n",
        "        DNAME = \" and \".join([str(x), str(y)])\n",
        "        OK = np.logical_and(~np.isnan(x), ~np.isnan(y))\n",
        "        x = np.asarray(x, dtype=np.int64)[OK]\n",
        "        y = np.asarray(y, dtype=np.int64)[OK]\n",
        "        r = np.unique(x).shape[0]\n",
        "        if r < 2 or np.unique(y).shape[0] != r:\n",
        "            raise ValueError(\"'x' and 'y' must have the same number of levels (minimum 2)\")\n",
        "        x = np.asarray(np.histogram2d(x, y, bins=r)[0], dtype=np.int64)\n",
        "\n",
        "    # get the marginals\n",
        "    rowsums = x.sum(axis=1)\n",
        "    colsums = x.sum(axis=0)\n",
        "    #  If you have perfect agreement then you want something along the lines of:\n",
        "    equalsums_rows = np.diag(x) == rowsums\n",
        "    equalsums_cols = np.diag(x) == colsums\n",
        "\n",
        "    equalsums = np.logical_and(equalsums_rows, equalsums_cols)\n",
        "\n",
        "    if equalsums.any():\n",
        "        # dump any categories with perfect agreement\n",
        "        x = x[~equalsums][:, ~equalsums]\n",
        "        # bail out if too many categories have disappeared\n",
        "        if x.shape[0] < 2:\n",
        "            raise ValueError(\"Too many equal marginals, cannot compute\")\n",
        "        # get new marginals\n",
        "        rowsums = x.sum(axis=1)\n",
        "        colsums = x.sum(axis=0)\n",
        "\n",
        "    # use K-1 marginals\n",
        "    Kminus1 = rowsums.shape[0] - 1\n",
        "    smd = rowsums[:-1] - colsums[:-1]\n",
        "    smS = np.zeros((Kminus1, Kminus1))\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = pd.DataFrame(x)\n",
        "    for i in range(Kminus1):\n",
        "        for j in range(Kminus1):\n",
        "            if i == j:\n",
        "                smS[i,j] = rowsums[i] + colsums[j] - 2 * x.iloc[i,i]\n",
        "            else:\n",
        "                smS[i,j] = -(x.iloc[i,j] + x.iloc[j,i])\n",
        "\n",
        "    N = rowsums.sum()\n",
        "\n",
        "    if BhapkarTest == False:\n",
        "        STATISTIC = smd @ np.linalg.inv(smS) @ smd\n",
        "        PARAMETER = r - 1\n",
        "        METHOD = \"Stuart-Maxwell test\"\n",
        "        # PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "        PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "\n",
        "    elif BhapkarTest == True:\n",
        "        STATISTIC_pre = smd @ np.linalg.inv(smS) @ smd\n",
        "        STATISTIC = STATISTIC_pre / ( 1- STATISTIC_pre / N)\n",
        "        PARAMETER = r - 1\n",
        "        METHOD = \"Bhapkar test\"\n",
        "        PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "\n",
        "\n",
        "    RVAL = {\"statistic\": STATISTIC, \"parameter/dof\": PARAMETER,\n",
        "            \"p.value\": PVAL, \"method\": METHOD, \"data.name\": DNAME,\n",
        "            \"N\": N}\n",
        "    return RVAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEAKXS9r5hro",
        "outputId": "147273df-8655-46b7-a595-2a3a2af5ba75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'kappa': 0.36619058036690944, 'std': 0.021343131937481063, 'z': 17.157302941272434, 'p': 5.542857116529104e-66}\n",
            "{'kappa': 0.39708280835735976, 'std': 0.022509121238915804, 'z': 17.640973369979772, 'p': 1.193866502429738e-69}\n"
          ]
        }
      ],
      "source": [
        "# Compute Kohen's K\n",
        "KAPPA = kappa(contingency_table)\n",
        "print(KAPPA['Unweighted'])\n",
        "print(KAPPA['Weighted'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STTLM_fi_dsm",
        "outputId": "5bcf9569-7e64-4509-91fa-cb9eb783d308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'statistic': 2.5000295037469757, 'parameter/dof': 2, 'p.value': 0.28650057040884724, 'method': 'Stuart-Maxwell test', 'data.name': 'x', 'N': 1200}\n",
            "{'statistic': 2.5052488337458025, 'parameter/dof': 2, 'p.value': 0.28575387463462465, 'method': 'Bhapkar test', 'data.name': 'x', 'N': 1200}\n"
          ]
        }
      ],
      "source": [
        "# Stuart-Maxwell test\n",
        "contingency_table = contingency_table.to_numpy()\n",
        "SM = StuartMaxwellTest(contingency_table, BhapkarTest=False)\n",
        "print(SM)\n",
        "# Bhapkar test\n",
        "BP = StuartMaxwellTest(contingency_table, BhapkarTest=True)\n",
        "print(BP)"
      ]
    }
  ]
}